\chapter{Conclusion}
The AK-MCS algorithm proved to have several advantages, being a substantial
improvement of the basic MCS method. It retains the advantages of the latter,
improving its main disadvantage which is having to call the expensive performance
function so many times. \\

The choice of one learning function over another should be influenced by the
nature and form of the problem. Although they are all based on the same parameters,
the manner in which they determine the evolution of the prediction process varies
by giving greater importance to different aspects of the design space and the performance
function. Likewise, the choice of thresholds for the stopping conditions is of great importance,
as they represent the usual trade-off between cost and benefit. It could be seen that in
the worked examples, the suggested threshold of the learning function H was too high,
yet for some particular problems it was low enough. \\

Given the simplicity of the nature of the method, its potential for improvement
is very high. In principle, the formulation of new learning functions is a
boundless task. Moreover, different ways of approaching the problem following the
same principles have been studied. For example, in \citep{Peijuan2017} geometrical considerations
are presented that allow to improve the efficiency of the method, although limiting
its applicability. In \citep{Balesdent2013} work is done on a variant of the crude MCS. And as well
as these, there are many modifications that are pending to be studied, and even
proposed, that could further improve the qualities of the method.